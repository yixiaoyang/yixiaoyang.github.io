---
author: leon
comments: true
date: 2018-06-16 13:20:00+00:00
layout: post
title: '[机器学习]傻逼如我重新分析交通事故数据并建立预测模型(二) - RNN神经网络模型'
categories:
- 机器学习
tags:
- 机器学习
- 神经网络
---

递归神经网络主要有两类：一种是时间上的递归神经网络，一种是结构上的递归神经网络。RNN主要解决序列数据的处理，如文本、语音、视频等等。这类数据存在顺序关系，每个样本和它之前的样本存在关联。

## Recurrent Neurons

**Equation 1:** output of sigle recurrent neuron for a single instance

$$
y_{(t)} = \phi(x_{(t)}^{T} \cdot w_x + y_{t-1}^T \cdot w_y + b)
$$

- $$ \phi $$ 是激活函数，如ReLU，tanh。

**Equation 2:** outputs of a layer of recurrent neurons for all instances in a mini-batch

$$
W = [\frac{W_x}{W_y}] \\
Y_{(t)} = \phi (X_{(t)} \cdot W_x + Y_{(t-1)} \cdot W_y + b)  = \phi ([X_{(t)}Y_{(t-1)}] \cdot W + b) 
$$

- $$Y_{(t)}$$ 是 $$m \times n_{neurons}$$ 的矩阵，表示的是step t时刻的输出。
- $$X_{(t)}$$ 是 $$m \times n_{inputs}$$ 的输入矩阵, $$n_{inputs}$$是输入feature的数量。
- $$W_x$$ 是 $$ n_{neurons} \times n_{inputs}$$大小，当前时间对的inputs权重矩阵。
- $$W_y$$ 是 $$ n_{neurons} \times n_{neurons}$$大小，上一时间output对当前输出的权重矩阵。


## RNN and HMM

RNN的本质是一个数据推断（inference）机器， 只要数据足够多，就可以得到从$$x_{(t}$$到$$y_{(t)}$$的概率分布函数， 寻找到两个时间序列之间的关联，从而达到推断和预测的目的。这个很容易联想到另一个大名鼎鼎的模型-隐马尔科夫模型HMM（Hidden Markow Model）。z两者都是根据输入x推断输出y并维护一个隐变量，区别在于迁跃策略。RNN使用神经元连接矩阵，通过时间展开（时间递归部分，前向传播）和时间上反向传播进行迭代。HMM则使用更新状态转移矩阵来进行时间上的迭代。


>HMM有三个典型(canonical)问题:  
>- 预测(filter)：已知模型参数和某一特定输出序列，求最后时刻各个隐含状态的概率分布, 通常使用前向算法解决. 即求 
$$P(x(t)\ |\ y(1),... ,y(t))$$ .
>- 平滑(smoothing)：已知模型参数和某一特定输出序列，求中间时刻各个隐含状态的概率分布，通常使用forward-backward 算法解决. 即求 
$$ P(x(k)\ |\ y(1),... ,y(t)), (k<t)$$ .
>- 解码(most likely explanation): 已知模型参数，寻找最可能的能产生某一特定输出序列的隐含状态的序列, 通常使用Viterbi算法解决. 即求 
$$ P([x(1)\dots x(t)]|[y(1)\dots ,y(t)])$$ .


## Backpropagation Through Time(BPTT)

RNN的训练类似于ANN，也可以使用反向传播算法。首先，通过整个网络前向传播; 然后计算输出序列上的损失和梯度，损失函数表示为：$$C(Y_{(t_min)}, Y_{(t_min+1)}, ... ,Y_{(t_max)} )$$（仅计算$$t_{min}$$和$$t_{max}$$时间段）; 接着将梯度反向传播到网络上所有的神经元上（不仅仅是最后一层上的神经元，计算损失和梯度相关时间上的神经元都需要更新）。

## LSTM Cells

LSTM Cell从外部看类似于Basic Cell，但是内部多了一些控制门，表现比Basic Cell要好。

## Gradient Vanishing / Gradient Exploding

RNN训练困难的主要原因再与隐藏层参数w传播过程时，无论在前向传播还是反向传播都会乘上多次。

> 许多人造神经网络（包括第一代RNNs网络），最终都遭受了严重的挫折——梯度消失问题。什么是梯度消失问题呢，其基本思想其实很简单。首先，来看一个梯度的概念，将梯度视为斜率。在训练深层神经网络的背景中，梯度值越大代表坡度越陡峭，系统能够越快地下滑到终点线并完成训练。但这也是研究者陷入困境的地方——当斜坡太平坦时，无法进行快速的训练。这对于深层网络中的第一层而言特别关键，因为若第一层的梯度值为零，说明没有了调整方向，无法调整相关的权重值来最下化损失函数，这一现象就是“消梯度失”。随着梯度越来越小，训练时间也会越来越长，类似于物理学中的沿直线运动，光滑表面，小球会一直运动下去。

## 参考
- https://en.wikipedia.org/wiki/Recurrent_neural_network
- https://feisky.xyz/machine-learning/rnn/